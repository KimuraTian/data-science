{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression, Naive Bayesian Classifier, and kNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define logistic function\n",
    "def logistic_func(x, theta):\n",
    "    # compute z\n",
    "    z = np.dot(x, theta)\n",
    "    logistic_value = 1.0 / (np.exp(-z) + 1)\n",
    "    return logistic_value\n",
    "\n",
    "# Define gradient descent function\n",
    "# alpha is importance factors, set it to ones when all data points are equally important\n",
    "def log_gradient(theta, x, y, alpha):\n",
    "    logistic_value = logistic_func(x, theta)\n",
    "    gradient = np.dot(np.squeeze(alpha) * np.transpose(x),  logistic_value - y)\n",
    "    return gradient\n",
    "\n",
    "# Define the cost function\n",
    "# alpha is importance factors, set it to ones when all data points are equally important\n",
    "def cost_func(theta, x, y, alpha):\n",
    "    log_func_v = logistic_func(x,theta)\n",
    "    step1 = y * np.log(log_func_v)\n",
    "    step2 = (1-y) * np.log(1 - log_func_v)\n",
    "    final = (-step1 - step2) * alpha\n",
    "    return np.mean(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 1, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 1, 1], [1, 1, 0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([[1],[1],[1],[0],[0]])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0],\n",
       "       [1, 1, 0, 0],\n",
       "       [1, 0, 0, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert ones into the first column of X to include constant term b in the formula\n",
    "X_b = np.insert(X, [0], np.ones((5, 1)), axis=1)\n",
    "X_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define gradient descent process\n",
    "def gradient_descent(theta, x, y, alpha, lc=0.001, stoppingCost=0.1, epsilon=0.001):\n",
    "    # number of iteration\n",
    "    i=0\n",
    "    # compute initial cost value\n",
    "    pre_cost = cost_func(theta, x, y, alpha) \n",
    "    print('initial stage: iteration=',i,' w1=',theta[1, 0],' w2=',theta[2, 0],' w3=',theta[3, 0],' b=',theta[0, 0], ' cost=', pre_cost)\n",
    "    # stopping condition: the cost is close to the stoppingCost\n",
    "    while abs(pre_cost-stoppingCost) > epsilon:\n",
    "        i = i + 1\n",
    "        # compute derivatives\n",
    "        d_theta = log_gradient(theta, x, y, alpha)\n",
    "        # update theta\n",
    "        theta = theta - lc * d_theta\n",
    "        # compute current cost\n",
    "        current_cost = cost_func(theta, x, y, alpha)\n",
    "        pre_cost = current_cost\n",
    "    print('final stage: iteration=',i,' w1=',theta[1, 0],' w2=',theta[2, 0],' w3=',theta[3, 0],' b=',theta[0, 0], ' cost=', pre_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three different initial points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial stage: iteration= 0  w1= 0.024406095618  w2= 0.273502828378  w3= 0.976566705746  b= 0.968976960363  cost= 0.912319176777\n",
      "final stage: iteration= 26483  w1= -3.62344159872  w2= -3.62103338054  w3= -1.37407582517  b= 5.4995054592  cost= 0.100999218101\n"
     ]
    }
   ],
   "source": [
    "alpha = np.ones((5, 1))\n",
    "theta = np.random.random_sample((4, 1))\n",
    "gradient_descent(theta, X_b, y, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial stage: iteration= 0  w1= 0.99393060029  w2= 0.915542728603  w3= 0.988356863571  b= 0.69661849635  cost= 1.3640223728\n",
      "final stage: iteration= 27667  w1= -3.62044559615  w2= -3.62102454028  w3= -1.38347433045  b= 5.49872340188  cost= 0.100999118443\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.random_sample((4, 1))\n",
    "gradient_descent(theta, X_b, y, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial stage: iteration= 0  w1= 0.209748854756  w2= 0.29612039617  w3= 0.345735839439  b= 0.455795664615  cost= 0.800768871087\n",
      "final stage: iteration= 26854  w1= -3.61986312752  w2= -3.61908924796  w3= -1.3914751938  b= 5.49808331879  cost= 0.100998830289\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.random_sample((4, 1))\n",
    "gradient_descent(theta, X_b, y, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three different initial points with importance factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial stage: iteration= 0  w1= 0.309876956707  w2= 0.732881480595  w3= 0.0339833709998  b= 0.637105926026  cost= 2.62501548653\n",
      "final stage: iteration= 30643  w1= -5.35113029025  w2= -5.35097313351  w3= -2.02518692491  b= 7.92457872494  cost= 0.100998462143\n"
     ]
    }
   ],
   "source": [
    "# Importance factors\n",
    "alpha = np.array([[2],[2],[2],[3],[3]])\n",
    "theta = np.random.random_sample((4, 1))\n",
    "gradient_descent(theta, X_b, y, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial stage: iteration= 0  w1= 0.418558531706  w2= 0.871097414493  w3= 0.538385337814  b= 0.789539332478  cost= 3.19920541762\n",
      "final stage: iteration= 30726  w1= -5.35154443579  w2= -5.35138400404  w3= -2.02143423084  b= 7.92491842444  cost= 0.100997063566\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.random_sample((4, 1))\n",
    "gradient_descent(theta, X_b, y, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial stage: iteration= 0  w1= 0.810785041115  w2= 0.676980136432  w3= 0.331019055677  b= 0.934262425502  cost= 3.42973928161\n",
      "final stage: iteration= 30676  w1= -5.35118272205  w2= -5.35123185163  w3= -2.02372008151  b= 7.92470310257  cost= 0.100998187549\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.random_sample((4, 1))\n",
    "gradient_descent(theta, X_b, y, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that in both cases the final weights and cost converge to the same values (within epsilon range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_vs_others(X, y):\n",
    "    # get all unique classes\n",
    "    classes = list(set(y))\n",
    "    # number of classes\n",
    "    nclasses = len(classes)\n",
    "    # create a model list\n",
    "    models = []\n",
    "    # loop through all classes and create different one vs others models\n",
    "    for i in range(nclasses):\n",
    "        # get binary labels (one vs others)\n",
    "        y_bi = np.array([1 if label == classes[i] else 0 for label in y])\n",
    "        # train Logistic Regression model\n",
    "        from sklearn import linear_model\n",
    "        clf = linear_model.LogisticRegression()\n",
    "        clf.fit(X, y_bi)\n",
    "        models.append(clf)\n",
    "    return classes, models\n",
    "\n",
    "def predict_one_vs_others(X, models, classes):\n",
    "    prediction_list = []\n",
    "    # go through all the models\n",
    "    for model in models:\n",
    "        # get the probability of predicting class 1, \n",
    "        # which means how likely the current sample is to be the class 1 of the current model \n",
    "        # and convert it to a column vector \n",
    "        prediction = model.predict_proba(X)[:, -1].reshape(-1, 1)\n",
    "        prediction_list.append(prediction)\n",
    "    # combine all predictions from different classifiers\n",
    "    pred_proba = np.hstack(prediction_list)\n",
    "    # choose the one with the highest prediction probability as the final prediction class\n",
    "    predictions_numeric = np.argmax(pred_proba, axis=1)\n",
    "    # convert back to original class labels\n",
    "    predictions = np.array([classes[i] for i in predictions_numeric])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of one vs others logistic regression= 1.0\n",
      "accuracy of one vs others logistic regression= 0.966666666667\n",
      "accuracy of one vs others logistic regression= 0.933333333333\n",
      "accuracy of one vs others logistic regression= 0.9\n",
      "accuracy of one vs others logistic regression= 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    X_train=X[train_index]\n",
    "    y_train=y[train_index]\n",
    "    X_test=X[test_index]\n",
    "    y_test=y[test_index]\n",
    "    \n",
    "    classes, models = train_one_vs_others(X_train, y_train)\n",
    "    predictions = predict_one_vs_others(X_test, models, classes)\n",
    "    \n",
    "    print('accuracy of one vs others logistic regression=',np.mean(predictions == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_vs_one(X, y):\n",
    "    # get all unique classes\n",
    "    classes = list(set(y))\n",
    "    # get all one vs one pair in the classes\n",
    "    import itertools\n",
    "    pairs = list(itertools.combinations(classes, 2))\n",
    "    # number of classes\n",
    "    nclasses = len(classes)\n",
    "    # create a model list\n",
    "    models = []\n",
    "    # loop through all pairs and create different one vs one models\n",
    "    for pair in pairs:\n",
    "        # get one vs one data samples (the samples with labels contained in the pair)\n",
    "        train_X = X[(y == pair[0]) | (y == pair[1])]\n",
    "        train_y = y[(y == pair[0]) | (y == pair[1])]\n",
    "        # convert label to binary label to train\n",
    "        # pair[0] is 0, pair[1] is 1\n",
    "        y_bi = np.array([0 if label == pair[0] else 1 for label in train_y])\n",
    "        # train Logistic Regression model\n",
    "        from sklearn import linear_model\n",
    "        clf = linear_model.LogisticRegression()\n",
    "        clf.fit(train_X, y_bi)\n",
    "        models.append(clf)\n",
    "    return pairs, models\n",
    "\n",
    "def predict_one_vs_one(X, models, pairs):\n",
    "    prediction_list = []\n",
    "    # nmodels must be equal to npairs\n",
    "    nmodels = len(models)\n",
    "    # go through all the models\n",
    "    for i in range(nmodels):\n",
    "        # get class pair and model\n",
    "        pair = pairs[i]\n",
    "        model = models[i]\n",
    "        prediction_bi = model.predict(X)\n",
    "        # convert back to original class in order to do a majority vote\n",
    "        prediction = np.array([pair[label] for label in prediction_bi]).reshape(-1, 1)\n",
    "        prediction_list.append(prediction)\n",
    "    # combine all predictions from different classifiers\n",
    "    pred_classes = np.hstack(prediction_list)\n",
    "    # choose the one with the highest count as the final prediction class\n",
    "    from collections import Counter\n",
    "    predictions = np.array([Counter(pred).most_common(1)[0][0] for pred in pred_classes])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of one vs others logistic regression= 1.0\n",
      "accuracy of one vs others logistic regression= 1.0\n",
      "accuracy of one vs others logistic regression= 0.933333333333\n",
      "accuracy of one vs others logistic regression= 0.966666666667\n",
      "accuracy of one vs others logistic regression= 1.0\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    X_train=X[train_index]\n",
    "    y_train=y[train_index]\n",
    "    X_test=X[test_index]\n",
    "    y_test=y[test_index]\n",
    "    \n",
    "    pairs, models = train_one_vs_one(X_train, y_train)\n",
    "    predictions = predict_one_vs_one(X_test, models, pairs)\n",
    "    \n",
    "    print('accuracy of one vs others logistic regression=',np.mean(predictions == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both procedures have high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_join_probability(X_train, y_train, X_test, h_test):\n",
    "    likelihood = 1\n",
    "    # go through all features in the sample X_test\n",
    "    for i in range(len(X_test)):\n",
    "        # get the ith column in X_train\n",
    "        train_feature = X_train[:, i]\n",
    "        # get required feature value\n",
    "        test_feature = X_test[i]\n",
    "        # compute probability of p(x | y) \n",
    "        count_x_y = len(X_train[(train_feature == test_feature) & (y_train == h_test)])\n",
    "        count_y = len(y_train[y_train == h_test])\n",
    "        p_x_y = count_x_y / count_y\n",
    "        # update likelihood\n",
    "        likelihood = likelihood * p_x_y\n",
    "    # compute the probability of y == h_test\n",
    "    p_y = len(y_train[y_train == h_test]) / len(y_train)\n",
    "    # multiply the likelihood by the probability of y == h_test\n",
    "    likelihood = likelihood * p_y\n",
    "    return likelihood\n",
    "\n",
    "def predict_naive_bayes_classifier(X_train, y_train, X_test):\n",
    "    h_0 = compute_join_probability(X_train, y_train, X_test, 0)\n",
    "    h_1 = compute_join_probability(X_train, y_train, X_test, 1)\n",
    "    prediction = 0\n",
    "    # compare two hypothesis\n",
    "    if h_1 > h_0:\n",
    "        prediction = 1\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction of data  [1 0 0] is  1 the actual label is  0\n",
      "the prediction of data  [1 0 1] is  1 the actual label is  1\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 1, 1], [1, 1, 0]])\n",
    "y_train = np.array([1, 1, 1, 0, 0])\n",
    "\n",
    "X_test1 = np.array([1, 0, 0])\n",
    "X_test2 = np.array([1, 0, 1])\n",
    "\n",
    "prediction = predict_naive_bayes_classifier(X_train, y_train, X_test1)\n",
    "print(\"the prediction of data \", X_test1, \"is \", prediction, \"the actual label is \", 0)\n",
    "\n",
    "prediction = predict_naive_bayes_classifier(X_train, y_train, X_test2)\n",
    "print(\"the prediction of data \", X_test2, \"is \", prediction, \"the actual label is \", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to compute the Euclidean distance between each training sample with a given test sample\n",
    "def compute_Euclidean_distance(X_train, X_test):\n",
    "    import numpy as np\n",
    "    return np.linalg.norm(X_train - X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_kNN(X_train, y_train, X_test, k, distance_metric=compute_Euclidean_distance):\n",
    "    # compute the distances\n",
    "    distances = distance_metric(X_train, X_test)\n",
    "    # get the indices of the data which have the first k shortest distance from X_test\n",
    "    k_nearest_indices = np.argsort(distances)[:k]\n",
    "    # get the corresponding labels\n",
    "    k_nearest_labels = y_train[k_nearest_indices]\n",
    "    # do a majority vote\n",
    "    from collections import Counter\n",
    "    return Counter(k_nearest_labels).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by using norm-2 distance, the  1 - NN model predictions are:  1 , 1\n",
      "by using norm-2 distance, the  1 - NN model accuracy is:  0.5\n",
      "by using norm-2 distance, the  3 - NN model predictions are:  1 , 1\n",
      "by using norm-2 distance, the  3 - NN model accuracy is:  0.5\n",
      "by using norm-2 distance, the  5 - NN model predictions are:  1 , 1\n",
      "by using norm-2 distance, the  5 - NN model accuracy is:  0.5\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 1, 1], [1, 1, 0]])\n",
    "y_train = np.array([1, 1, 1, 0, 0])\n",
    "\n",
    "X_test1 = np.array([1, 0, 0])\n",
    "X_test2 = np.array([1, 0, 1])\n",
    "y_test1 = 0\n",
    "y_test2 = 1\n",
    "\n",
    "# for each odd number of k less than the sample size\n",
    "# do prediction\n",
    "for k in range(1, len(X_train) + 1, 2):\n",
    "    pred1 = predict_kNN(X_train, y_train, X_test1, k)\n",
    "    pred2 = predict_kNN(X_train, y_train, X_test2, k)\n",
    "    print(\"by using norm-2 distance, the \", k, \"- NN model predictions are: \", pred1, \",\",pred2)\n",
    "    # compute accuracy\n",
    "    accuracy = np.mean([pred1==y_test1, pred2==y_test2])\n",
    "    print(\"by using norm-2 distance, the \", k, \"- NN model accuracy is: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
